{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Zhewei Yao <https://github.com/yaozhewei>, Amir Gholami <http://amirgholami.org/>\n",
    "\n",
    "\n",
    "This tutorial shows how to compute the Hessian information using (randomized) numerical linear algebra for both explicit Hessian (the matrix is given) as well as implicit Hessian (the matrix is ungiven).\n",
    "\n",
    "We'll start by doing the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/envs/nico_msc/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torchvision import datasets, transforms\n",
    "from pyhessian import hessian # Hessian computation\n",
    "from density_plot import get_esd_plot # ESD plot\n",
    "from pytorchcv.model_provider import get_model as ptcv_get_model # model\n",
    "from models import get_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable cuda devices\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(name='cifar10', train_bs=128, test_bs=1000):\n",
    "    \"\"\"\n",
    "    Get the dataloader\n",
    "    \"\"\"\n",
    "    if name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        trainset = datasets.CIFAR10(root='../data',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=train_bs,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        testset = datasets.CIFAR10(root='../data',\n",
    "                                   train=False,\n",
    "                                   download=False,\n",
    "                                   transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=test_bs,\n",
    "                                                  shuffle=False)\n",
    "    if name == 'cifar10_without_dataaugmentation':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        trainset = datasets.CIFAR10(root='../data',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=train_bs,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        testset = datasets.CIFAR10(root='../data',\n",
    "                                   train=False,\n",
    "                                   download=False,\n",
    "                                   transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=test_bs,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def test(model, test_loader, cuda=True):\n",
    "    \"\"\"\n",
    "    Get the test performance\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total_num = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        output = model(data)\n",
    "        pred = output.data.max(\n",
    "            1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "        total_num += len(data)\n",
    "    print('testing_correct: ', correct / total_num, '\\n')\n",
    "    return correct / total_num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Power Iteration for NN Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepee import ModelSurgeon\n",
    "from deepee.surgery import SurgicalProcedures\n",
    "from functools import partial\n",
    "surgeon = ModelSurgeon(partial(SurgicalProcedures.BN_to_GN, num_groups=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model \n",
    "architecture = 'efficientnetb0'\n",
    "cuda = False\n",
    "if architecture=='densenet':\n",
    "    model=get_model(\"densenet121\", False, 10, \"imagenette\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "    model = surgeon.operate(model)\n",
    "    dense_dict = torch.load(\"/home/alex/DPBenchmark/densenet121_gn_8_imagenette_eps7.pt\")\n",
    "    dense_dict = {k.replace(\"_module.\", \"\"):v for k,v in dense_dict.items()}\n",
    "    model.load_state_dict(dense_dict)\n",
    "elif architecture == 'smoothnet':\n",
    "    model = get_model('en_scaling_residual_model', False, 10, 'imagenette', 3, [16, 32], 1, 5, 8, True, 'mxp_gn', 'selu', 2, True, False)\n",
    "    smooth_dict = {k.replace(\"_module.\", \"\"):v for k,v in torch.load(\"smoothnetw80d50_imagenette_eps7.pt\", map_location='cpu').items()}\n",
    "    model.load_state_dict(smooth_dict) \n",
    "elif architecture=='resnet34':\n",
    "    model=get_model(\"resnet34\", False, 10, \"imagenette\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "    model = surgeon.operate(model)\n",
    "    model.fc = torch.nn.Linear(512, 10)\n",
    "    res_dict = torch.load(\"/home/alex/DPBenchmark/resnet34_gn8_imagenette_eps7.pt\")\n",
    "    res_dict = {k.replace(\"_module.\", \"\"):v for k,v in res_dict.items()}\n",
    "    model.load_state_dict(res_dict)\n",
    "elif architecture=='efficientnetb0':\n",
    "    import timm\n",
    "    model = timm.create_model(\"efficientnet_b0\")\n",
    "    # model=get_model(\"efficientnet_b0\", False, 10, \"imagenette\", 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "    # NOTE: b0 was 1280, b3 was 1536, b5 was 2048, b7 was 2560\n",
    "    model.classifier = torch.nn.Linear(1280, 10)\n",
    "    model = surgeon.operate(model)\n",
    "    eff_dict = torch.load(\"efficientb0_gn8_imagenette_eps7.pt\")\n",
    "    eff_dict = {k.replace(\"_module.\", \"\"):v for k,v in eff_dict.items()}\n",
    "    model.load_state_dict(eff_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# create loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# get dataset \n",
    "train_loader, test_loader = getData(train_bs=1000)\n",
    "\n",
    "# for illustrate, we only use one batch to do the tutorial\n",
    "for inputs, targets in train_loader:\n",
    "    break\n",
    "\n",
    "# we use cuda to make the computation fast\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hessian computation module\n",
    "hessian_comp = hessian(model, criterion, data=(inputs, targets), cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top Hessian eigenvalue of this model is 850.7018\n"
     ]
    }
   ],
   "source": [
    "# Now let's compute the top eigenvalue. This only takes a few seconds.\n",
    "top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(maxIter=1000)\n",
    "print(\"The top Hessian eigenvalue of this model is %.4f\"%top_eigenvalues[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's compute the top 2 eigenavlues and eigenvectors of the Hessian\n",
    "top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(top_n=2, maxIter=1000)\n",
    "print(\"The top two eigenvalues of this model are: %.4f %.4f\"% (top_eigenvalues[-1],top_eigenvalues[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The small difference between this top eigenvalue (195.4954) and the previous one (195.5897) is due to the small number of iterations that we used in Power iteration. You can remove this small difference by increasing the number of iterations for power iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2.1: Plot Loss Landscape\n",
    "\n",
    "We can use the Hessian eigenvectors/eigenvalues to analyze the flat/sharpness of the loss landscape of your model, and plot the loss landscape. We will show that this can be more informative than using random directions.\n",
    "\n",
    "To plot the loss landscape, we first compute the top Hessian eigenvector and then perturb the model parameters along that direction and measure the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top eigenvector\n",
    "top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple function, that will allow us to perturb the model paramters and get the result\n",
    "def get_params(model_orig,  model_perb, direction, alpha):\n",
    "    for m_orig, m_perb, d in zip(model_orig.parameters(), model_perb.parameters(), direction):\n",
    "        m_perb.data = m_orig.data + alpha * d\n",
    "    return model_perb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda is a small scalar that we use to perturb the model parameters along the eigenvectors \n",
    "lams = np.linspace(-0.5, 0.5, 21).astype(np.float32)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# create a copy of the model\n",
    "model_perb.eval()\n",
    "model_perb = model_perb.cuda()\n",
    "\n",
    "for lam in lams:\n",
    "    model_perb = get_params(model, model_perb, top_eigenvector[0], lam)\n",
    "    loss_list.append(criterion(model_perb(inputs), targets).item())\n",
    "\n",
    "plt.plot(lams, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Perturbation')\n",
    "plt.title('Loss landscape perturbed based on top Hessian eigenvector')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare this with a loss landscape computed based on perturbing the model parameters along a random direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian.utils import normalization\n",
    "\n",
    "# generate random vector to do the loss plot\n",
    "\n",
    "v = [torch.randn_like(p) for p in model.parameters()]\n",
    "v = normalization(v)\n",
    "\n",
    "\n",
    "# used to perturb your model \n",
    "lams = np.linspace(-0.5, 0.5, 21).astype(np.float32)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# create a copy of the model\n",
    "model_perb = ptcv_get_model(\"resnet20_cifar10\", pretrained=True)\n",
    "model_perb.eval()\n",
    "model_perb = model_perb.cuda()\n",
    "\n",
    "for lam in lams: \n",
    "    model_perb = get_params(model, model_perb, v, lam)\n",
    "    loss_list.append(criterion(model_perb(inputs), targets).item())\n",
    "\n",
    "plt.plot(lams, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Perturbation')\n",
    "plt.title('Loss landscape perturbed based on a random direction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how different the loss landscape looks. In particular note that there is almost no change in the loss value (see the small scale of the y-axis). This is expected, since for a converged NN, many of the directions are typically degenarate (i.e. they are flat)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use gradient direction to perturb the model. While gradient is better than random vector, but it is not possible to use it to plot 3D loss landscape since you will need more than one direction. However, you can use top 2 Hessian vectors instead for that scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian.utils import normalization\n",
    "\n",
    "\n",
    "# used to perturb your model \n",
    "lams = np.linspace(-0.5, 0.5, 21).astype(np.float32)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# create a copy of the model\n",
    "model_perb = ptcv_get_model(\"resnet20_cifar10\", pretrained=True)\n",
    "model_perb.eval()\n",
    "model_perb = model_perb.cuda()\n",
    "\n",
    "# generate gradient vector to do the loss plot\n",
    "loss = criterion(model_perb(inputs), targets)\n",
    "loss.backward()\n",
    "\n",
    "v = [p.grad.data for p in model_perb.parameters()]\n",
    "v = normalization(v)\n",
    "model_perb.zero_grad()\n",
    "\n",
    "\n",
    "for lam in lams: \n",
    "    model_perb = get_params(model, model_perb, v, lam)\n",
    "    loss_list.append(criterion(model_perb(inputs), targets).item())\n",
    "\n",
    "plt.plot(lams, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Perturbation')\n",
    "plt.title('Loss landscape perturbed based on gradient direction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's repeate the above for computing the trace and diagonal of Hessian for ResNet20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# change the model to eval mode to disable running stats upate\n",
    "model.eval()\n",
    "\n",
    "# create loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# get dataset \n",
    "train_loader, test_loader = getData(train_bs=1000)\n",
    "\n",
    "# for illustrate, we only use one batch to do the tutorial\n",
    "for inputs, targets in train_loader:\n",
    "    break\n",
    "\n",
    "# we use cuda to make the computation fast\n",
    "cuda=False\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the hessian computation module\n",
    "hessian_comp = hessian(model, criterion, data=(inputs, targets), cuda=cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = hessian_comp.trace(maxIter=1000)\n",
    "print(\"The trace of this model is: %.4f\"%(np.mean(trace)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the full eigenvalue spectrum density of Hessian using Stochastic Lancoz algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "density_eigen, density_weight = hessian_comp.density()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_esd_plot(density_eigen, density_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above ESD plot is very interesting and shows that a lot of the eigenvalues of the Hessian are close to zero. This means that a lot of the directions along the loss landscape is almost flat. We expect this based on the loss landscape that we got above when we used a random direction. Another interesting observation is that there are several large Hessian outliers. The other very interesting finding, is that there are a lot of directions with slight negative curvature. This means that we still have not converged to a perfect local minimum that satisfies first and second order optimality conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
