###########
# General #
###########
project_name: "dp_benchmark" # baseline for no-DP experiments
experiment_name: "" # leave blank for sweep
including_test: true
max_epochs: 90
device: 'cuda:0'
use_all_gpus: false
print_every_iter: 50 # for training (validation after every epoch)

#########
# Model #
#########
# possible models: see model.py
  # resnet18, vgg11(_bn), inception_v4, densenet121, densenet201, wide_resnet50_2,
  # condense_net_v1, mobilenetv1_w1, mobilenetv1_w025, mobilenetv1_w050, mobilenetv1_w075
  # mobilenetv3_large_100, mobilenet_v3_small, vit_base_patch16_224,
  # simple_conv, stage_conv, en_scaling_base_model, en_scaling_residual_model, 
  # googlenet, xception, efficientnet_b[0-7]
  # any other model is tried to be created from the timm library
model_name: 'resnet9'
pretrained: False
weight_init: True # xavier_uniform (set correct activation function in utils.py)
weight_norm: False
# for SimpleConvNet and DepthConvNet
kernel_size: 3
conv_layers: [16, 32]
# for stage_conv
nr_stages: 1
# for en_scaling_residual_model
halve_dim: True
# possible choices: see utils.py
  # norms: batch_norm, group_norm, instance_norm, layer_norm
  # pooling: max_pool, avg_pool
  # combination: mxp_gn, mxp_ln
  # nothing: identity
after_conv_fc_str: 'mxp_gn'
# possible choices: see utils.py
# selu, relu, leaky_relu
activation_fc_str: 'selu'
skip_depth: 2
# for en_scaling_*
depth: 5.0
width: 8.0
# depthwise seperable convolutions
dsc: False
# for dense components
# skip_depth and halve_dim will be ignored
# width ~ growing factor
# depth = num of Conv-AfterConvFc-Activation triples per DenseBlock
dense: True

######## 
# Data #
######## 
data_name: 'imagenette' # or cifar10
data_root: '/u/home/remersch/DPBenchmark/data'
batch_size: 256
physical_batch_size: 64
num_classes: 10
val_split: 0.2

############# 
# Optimizer #
############# 
# possible optims: sgd, adam
optimizer: 'sgd'
lr: 0.002
momentum: 0.9
weight_decay: 0.0002
lr_scheduler: true

#############
# DP params #
#############
dp_tool: "opacus"
dp: true
L2_clip: 0.1
#noise_multiplier: 0.4272 
target_epsilon: 7.0
target_delta: 0.00001
abort: true

#######################
# experimental params #
#######################

### study trainings (short) ###
# sweep: sweeps/dp_params_sweep_study.yaml 
# lr: 0.1 if config.dp else 0.05
# lr-scheduler: step_size=1 if config.dp else 10, gamma=0.7,
# SGD without momentum and weight-decay
# batch-size: 128 if config.dp else 32
# weight-init: false
# patience on val-loss: 15
# divergence threshold on val-loss: 1000

### benchmark trainings (long) ###
# sweep: sweeps/dp_params_sweep_study.yaml
# lr: 2e-3 if config.dp else 0.05
# lr-scheduler: step_size=5 if config.dp else 10, gamma=0.9 if config.dp else 0.7
# SGD momentum: 0.9, weight-decay: 2e-4
# batch-size: 256 if config.dp else 32
# weight-init: xavier-glorot (relu or selu gain) if config.dp else false
# patience on val-loss: 30 if config.dp else 15
# divergence threshold on val-loss: 1000
# NOTE: no-DP case uses same params as for the study